{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "782da64b",
   "metadata": {},
   "source": [
    "IMPORT AND SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3282f131",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2 \n",
    "import itertools\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "try:\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "    print(\"✅ Op Determinism Abilitato!\")\n",
    "except AttributeError:\n",
    "    print(\"⚠️ Attenzione: La tua versione di TF è troppo vecchia per enable_op_determinism.\")\n",
    "\n",
    "def reset_seeds(seed=42):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "SEEDS = [555, 999]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed65e287",
   "metadata": {},
   "source": [
    "DATASET LOADING AND MODELLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d7a10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    folder='dataset/images'\n",
    "    data=[]\n",
    "    for filename in sorted(os.listdir(folder)):\n",
    "        img_path=os.path.join(folder,filename)\n",
    "        img=cv2.imread(img_path) #opencv save in bgr\n",
    "        data.append({\n",
    "            'image':img,\n",
    "            'filename':filename\n",
    "        })\n",
    "\n",
    "    print(len(data),'images loaded')\n",
    "    print('file name is: ',data[0]['filename'], 'shape of the image is:  ', data[0]['image'].shape )\n",
    "\n",
    "    label=pd.read_csv('dataset/raw/bbx_annotations.csv')\n",
    "    print(label.shape, label.iloc[0]['filename'])\n",
    "    #images order is random, and for 1 image you can have more class\n",
    "\n",
    "    print('we have', len(label['class'].unique()), 'different classes')\n",
    "\n",
    "    #replace biggger img with half sized ones\n",
    "    #cv2.imwrite('resize_image/last_img_pre_downsampling.jpg',data[-100]['image'])\n",
    "    for i,item in enumerate(data):\n",
    "        if \"upper\" in item[\"filename\"].lower():\n",
    "            data[i]['image']=cv2.resize(\n",
    "                data[i]['image'],\n",
    "                (data[i]['image'].shape[1]//2,data[i]['image'].shape[0]//2)\n",
    "                ,interpolation=cv2.INTER_AREA\n",
    "            )\n",
    "    #cv2.imwrite('resize_image/last_img_post_downsampling.jpg',data[-100]['image'])\n",
    "       \n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea259a97",
   "metadata": {},
   "source": [
    "FROM THE PURE DATASET TO THE TRAIN AND TEST DATA AND LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e35d0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_modelling(dataset,annotation):   \n",
    "    dataset_df = pd.DataFrame(dataset) \n",
    "    label_map={'goalpost':0,\n",
    "               'ball':1,\n",
    "               'robot':2,\n",
    "               'goalspot':3,\n",
    "               'centerspot':4}\n",
    "    def get_vector(classes_found):\n",
    "        vec=np.zeros(5,dtype=int)\n",
    "\n",
    "        for c in classes_found:\n",
    "            if c in label_map:\n",
    "                vec[label_map[c]]=1\n",
    "        return list(vec)\n",
    "    \n",
    "    grouped = annotation.groupby('filename')['class'].apply(list).reset_index()\n",
    "    grouped['label']=grouped['class'].apply(get_vector)\n",
    "    final_annotation=grouped[['filename','label']]\n",
    "\n",
    "    final_dataset= pd.merge(dataset_df, final_annotation[['filename', 'label']], on='filename', how='inner')\n",
    "    final_dataset.to_csv('csv/temp/final_dataset.csv')\n",
    "    final_dataset=final_dataset.drop(columns=['filename'])\n",
    "    df_train, df_test = train_test_split(final_dataset, test_size=0.2, random_state=42)\n",
    "    x_train = np.array(df_train['image'].tolist()).astype('float32') /255.0\n",
    "    y_train = np.array(df_train['label'].tolist()).astype('float32')\n",
    "    \n",
    "    x_test = np.array(df_test['image'].tolist()).astype('float32') / 255.0\n",
    "    y_test = np.array(df_test['label'].tolist()).astype('float32')\n",
    "    return x_train, y_train,x_test,y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f862c6d",
   "metadata": {},
   "source": [
    "DOUBLING THE DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fa642e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_train_set(x_train,y_train,aug_type):\n",
    "    rng = np.random.RandomState(42)    \n",
    "    if aug_type=='flip':\n",
    "        x_flipped = np.flip(x_train, axis=2)\n",
    "        y_flipped = y_train\n",
    "        x_train_aug = np.concatenate([x_train, x_flipped], axis=0)\n",
    "        y_train_aug = np.concatenate([y_train, y_flipped], axis=0)\n",
    "    elif aug_type=='noise':\n",
    "        noise = rng.normal(loc=0.0, scale=0.05, size=x_train.shape)\n",
    "        x_noisy = x_train + noise\n",
    "        x_noisy = np.clip(x_noisy, 0., 1.)\n",
    "        x_train_aug = np.concatenate([x_train, x_noisy], axis=0)\n",
    "        y_noise = y_train\n",
    "        y_train_aug = np.concatenate([y_train, y_noise], axis=0)\n",
    "    elif aug_type=='both':\n",
    "        x_flipped = np.flip(x_train, axis=2)\n",
    "        y_flipped = y_train\n",
    "        noise = rng.normal(loc=0.0, scale=0.05, size=x_train.shape)\n",
    "        x_noisy = x_train + noise\n",
    "        x_noisy = np.clip(x_noisy, 0., 1.)\n",
    "        y_noise = y_train\n",
    "        x_train_aug = np.concatenate([x_train,x_flipped, x_noisy], axis=0)\n",
    "        y_train_aug = np.concatenate([y_train,y_flipped, y_noise], axis=0)\n",
    "\n",
    "    #avoid to have all noisy data in validation--> shuffle\n",
    "    indices = np.arange(x_train_aug.shape[0])\n",
    "    rng.shuffle(indices)\n",
    "    x_train_aug = x_train_aug[indices]\n",
    "    y_train_aug = y_train_aug[indices]\n",
    "\n",
    "    return x_train_aug, y_train_aug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfecc971",
   "metadata": {},
   "source": [
    "CREATE ALL DIFFERENT COMBINATION OF HYPERPARAMETER, WITHOUT USING GRIDSEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e29b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hyperparam_combination():\n",
    "    param_grid = {\n",
    "    'batch_size': [16],\n",
    "    'layer_number':[4,5],\n",
    "    'kernel_dim': [7],\n",
    "    'pool_dim': [3], \n",
    "    'lr': [0.0001,0.001],\n",
    "    'fc1' : [128],\n",
    "    'fc2': [128]     \n",
    "}\n",
    "\n",
    "    #every possible combination\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    combinations = list(itertools.product(*values))\n",
    "    combinations_dicts = [dict(zip(keys, v)) for v in combinations]\n",
    "    return combinations_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b40f52",
   "metadata": {},
   "source": [
    "MODEL BUILIDNG, \n",
    "kernel dimesnsion, pooling dimension, fc layers dimension, number of conv layer and learning rate have different combination.\n",
    "instead, i fixed:\n",
    "pooling stride=2 \n",
    "pooling type: avg pooling\n",
    "number of kernel per layer: 16, 32, 64...\n",
    "last pooling: glob avg pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673a61ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(layer_num,kernel_dim,pool_dim,fc1,fc2):\n",
    "    model=models.Sequential()\n",
    "    model.add(tf.keras.Input(shape=(240,320,3)))\n",
    "\n",
    "    for i in range(layer_num):\n",
    "        kernel_number=16*(2**i)\n",
    "        model.add(layers.Conv2D(kernel_number,(kernel_dim,kernel_dim),activation='relu',padding='same'))\n",
    "        model.add(layers.AveragePooling2D((pool_dim,pool_dim),strides=2,padding='same'))\n",
    "    \n",
    "    model.add(layers.GlobalAveragePooling2D())\n",
    "    model.add(layers.Dense(fc1,activation='relu'))\n",
    "    model.add(layers.Dense(fc2,activation='relu'))\n",
    "    model.add(layers.Dense(5,activation='sigmoid'))\n",
    "\n",
    "    #model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7206a2",
   "metadata": {},
   "source": [
    "SAVING THE CSV FILE CONTAINING F1S OF EACH SEARCHED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f849a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saving_csv(report_dict,param,target_names,all_results,seed,aug_type):\n",
    "    current_result = param.copy()    \n",
    "    for class_name in target_names:\n",
    "        current_result[f'f1_{class_name}'] = report_dict[class_name]['f1-score']\n",
    "    current_result['f1_macro_avg'] = report_dict['macro avg']['f1-score']\n",
    "\n",
    "    all_results.append(current_result)\n",
    "    pd.DataFrame(all_results).to_csv(f'csv/f1_search/search6{aug_type}_{seed}.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8a2e85",
   "metadata": {},
   "source": [
    "MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee23710",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, annotation=load_dataset()\n",
    "x_train,y_train, x_test, y_test=dataset_modelling(dataset, annotation)\n",
    "list_augtype=['flip','noise','both']\n",
    "for aug_type in list_augtype:\n",
    "    x_train_aug, y_train_aug = augment_train_set(x_train, y_train,aug_type)\n",
    "\n",
    "    combination=create_hyperparam_combination()\n",
    "\n",
    "    for seed in SEEDS:\n",
    "        bestf1=0\n",
    "        all_results = []\n",
    "\n",
    "        print(f\"\\n ================== INIZIO CICLO CON SEED: {seed} ================== \")\n",
    "        for i,param in enumerate(combination):\n",
    "\n",
    "            \n",
    "            print(f\"\\nTRAINING RUN {i+1}/{len(combination)}\")\n",
    "            tf.keras.backend.clear_session()\n",
    "            reset_seeds(seed)\n",
    "            model=build_model( param['layer_number'],param['kernel_dim'],param['pool_dim'],param['fc1'],param['fc2'])\n",
    "            opt=tf.keras.optimizers.Adam(learning_rate=param['lr'])\n",
    "            model.compile(optimizer=opt,\n",
    "                        loss='binary_crossentropy',\n",
    "                        metrics=[tf.keras.metrics.Precision(name='precision'),\n",
    "                                tf.keras.metrics.Recall(name='recall'),\n",
    "                                ])\n",
    "            early_stop=EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True\n",
    "            )\n",
    "            reset_seeds(seed)\n",
    "            #class_weights_dict=compute_class_weight(y_train)\n",
    "            history=model.fit(\n",
    "                x_train_aug,y_train_aug,\n",
    "                epochs=120,\n",
    "                batch_size=param['batch_size'], \n",
    "                validation_split=0.2, \n",
    "                callbacks=[early_stop]\n",
    "                )\n",
    "\n",
    "            y_pred=model.predict(x_test)\n",
    "            predictions_binary = (y_pred > 0.5).astype(int)\n",
    "            target_names = ['goalpost','ball','robot','goalspot','centerspot']\n",
    "            report_dict = classification_report(y_test, predictions_binary, target_names=target_names, output_dict=True)\n",
    "            f1_macro = report_dict['macro avg']['f1-score']\n",
    "            if f1_macro>bestf1:\n",
    "                best_report_dict=report_dict\n",
    "                best_param=param\n",
    "                bestf1=f1_macro\n",
    "            saving_csv(report_dict,param,target_names,all_results,seed,aug_type)\n",
    "\n",
    "\n",
    "        df_report = pd.DataFrame(best_report_dict).transpose()\n",
    "        df_report = df_report.round(2)\n",
    "        df_report['support'] = df_report['support'].astype(int)\n",
    "        csv_path = f'csv/report/report_bestcomb6{aug_type}_{seed}.csv'\n",
    "        df_report.to_csv(csv_path)\n",
    "        print(best_param)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_hw2 (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
